{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.resources import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the random variable with a Gaussian distribution with mean $\\mu$ (`mu`) and variance $P$. We write its probability density function (pdf) as\n",
    "$$ p(x) = N(x|\\mu,P) = (2 \\pi P)^{-1/2} e^{-(x-\\mu)^2/2P} \\, . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Code it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['pdf_G_1'] = ['MD',r'''\n",
    "    return 1/sqrt(2*pi*P)*exp(-0.5*(xx-mu)**2/P)\n",
    "    #return sp.stats.norm.pdf(xx,loc=mu,scale=sqrt(P))\n",
    "''']\n",
    "show_answer('pdf_G_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Univariate (scalar), Gaussian pdf\n",
    "def pdf_G_1(xx,mu,P):\n",
    "    ### INSERT ANSWER ###\n",
    "    return 1/sqrt(2*pi*P)*exp(-0.5*(xx-mu)**2/P)\n",
    "    #return sp.stats.norm.pdf(xx,loc=mu,scale=sqrt(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu  = 0\n",
    "P   = 25    \n",
    "P12 = sqrt(P)\n",
    "\n",
    "xx = linspace(-20,20,201)\n",
    "plt.subplot(211)\n",
    "plt.plot(xx,pdf_G_1(xx,mu,P));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could for example be the density of a stochastic noise variable. It could also describe our uncertainty about a parameter (or state), which we model as randomness in the Bayesian paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines the pdf of the *multivariate* Gaussian. Take a moment to digest the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "def weighted_norm22(xx,W):\n",
    "    # W : the weighting matrix.\n",
    "    # xx: $N$-by-$m$, where m len(xx[n])==m for all n in 1...N.\n",
    "    return np.sum((xx @ W) * xx, axis=1)\n",
    "\n",
    "def pdf_G_m(xx,mu,P):\n",
    "    return 1/sqrt(det(2*pi*P))*exp(-0.5*weighted_norm22(xx-mu,inv(P)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots it (above) as contour (equi-density) curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_2_array(grid): return array([xi.ravel() for xi in grid]).T\n",
    "def square_reshape(X):  return X.reshape(int(sqrt(len(X))),-1)\n",
    "\n",
    "grid       = np.meshgrid(xx,xx)\n",
    "grid       = list_2_array(grid)\n",
    "pdf_values = pdf_G_m(grid, 0, P*array([[1,0.7],[0.7,1]]))\n",
    "pdf_values = square_reshape(pdf_values)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.contour(xx,xx,pdf_values);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' rule is how we do inference. For continuous random variables, $x$ and $y$, it reads:\n",
    "\n",
    "The \"posterior\" (pdf of $x$ given $y$) is the \"prior\" (pdf of $x$) times the \"likelihood\" (of $y$ given $x$), normalized.\n",
    "\n",
    "$$ p(x|y) = \\frac{p(x) \\, p(y|x)}{p(y)} \\, .$$\n",
    "\n",
    "**Exc:** Derive Bayes' rule from the definition of conditional pdf's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['BR deriv'] = ['MD',r'''\n",
    "[Wiki](https://en.wikipedia.org/wiki/Bayes%27_theorem#Derivation)\n",
    "''']\n",
    "show_answer('BR deriv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerically, the distributions may be represented by their values on a grid. Bayes' rule the consists of *(grid-)pointwise* multiplication, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values,lklhd_values,dx):\n",
    "    posterior_values = prior_values*lklhd_values\n",
    "    posterior_values /= sum(posterior_values)*dx # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Why does `Bayes_rule` not need to know the denominator, $p(y)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['BR grid normalization'] = ['MD',r'''\n",
    "Because $p(y) = \\int p(x,y) \\, dx = \\int p(x) p(y|x) \\, dx$, \n",
    "which is what gets computed by the sum over the grid values together with `dx`.\n",
    "''']\n",
    "show_answer('BR grid normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In higher dimensions, (pointwise) multiplication becomes a preposterious notion.\n",
    "\n",
    "**Exc:** How many point-multiplications are needed on a grid with $m_{grid}$ points in $d$ dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['num mult'] = ['MD',r'''\n",
    "$(m_{grid})^d$\n",
    "''']\n",
    "show_answer('num mult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 'Gaussian Bayes':\n",
    "Derive the expression for the posterior of a Gaussian prior $N(x|b,B)$ and a Gaussian likelihood $N(y|x,R)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['BR Gauss'] = ['MD',r'''\n",
    "We can ignore all factors that do not depend on $x$.\n",
    "\\begin{align}\n",
    "p(x|y) &= \\frac{p(x) \\, p(y|x)}{p(y)}\n",
    "\\propto p(x) \\, p(y|x) \\\\\\\n",
    "&\\propto N(x|b,B) \\, N(y|x,R) \\\\\\\n",
    "&\\propto \\exp \\Big( \\frac{-1}{2} \\Big( (x-b)^2/B + (x-y)^2/B\\Big) \\Big) \\\\\\\n",
    "&\\propto \\exp \\Big( \\frac{-1}{2} \\Big( (1/B + 1/R)x^2 - 2(b/B + y/R)x \\Big) \\Big) \\\\\\\n",
    "&\\propto \\exp \\Big( \\frac{-1}{2} \\Big( x - \\frac{b/B + y/R}{1/B + 1/R} \\Big)^2 \\cdot (1/B + 1/R) \\Big) \\\\\\\n",
    "&\\propto N(x|\\mu,P) \\, ,\n",
    "\\end{align}\n",
    "i.e., by identification,\n",
    "$ p(x|y) = N(x|\\mu,P) \\, ,$\n",
    "with\n",
    "\\begin{align}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\\\\\\n",
    "  \\mu &= P(b/B + y/R) \\, .\n",
    "\\end{align}\n",
    "''']\n",
    "show_answer('BR Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Use some algebra (only!) to show that\n",
    "\\begin{align}\n",
    "    P &= (1-K)B \\, , \\\\\\\n",
    "  \\mu &= b + K (y-x) \\, ,\n",
    "\\end{align}\n",
    "where $K = B/(B+R)$, which is called the \"Kalman gain\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Why is $K$ a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['KG 2'] = ['MD',r'''\n",
    " * Because it drags the estimate from $b$ \"towards\" $y$. Because it is beteen 0 and 1.\n",
    "   It weights the observation noise level (R) vs. the total noise level (B+R).\n",
    "   In the multivariate case, the same holds for its eigenvectors (if $H=I$).\n",
    "''']\n",
    "show_answer('KG 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc**: Write a Gaussian-Gaussian Bayes' rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['BR Gauss code'] = ['MD',r'''\n",
    "    P  = 1/(1/B+1/R)\n",
    "    mu = P*(b/B+y/R)\n",
    "    #     KG = B/(B+R)\n",
    "    #     P  = (1-KG)*B\n",
    "    #     mu = b + KG*(y-mu)\n",
    "''']\n",
    "show_answer('BR Gauss code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayes_rule_Gaussian(b,B,y,R):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    P  = 1/(1/B+1/R)\n",
    "    mu = P*(b/B+y/R)\n",
    "    #     KG = B/(B+R)\n",
    "    #     P  = (1-KG)*B\n",
    "    #     mu = b + KG*(y-mu)\n",
    "    return mu,P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below show's Bayes' rule in action on Gaussian pdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "b = 0\n",
    "B = 1\n",
    "@interact(y=(-10,10,1),log_R=(-2,5,0.5))\n",
    "def animate_Gaussian_Bayes(y=4.0,log_R=1):\n",
    "    R = exp(log_R)\n",
    "\n",
    "    prior     = lambda x: pdf_G_1(x,b,B)\n",
    "    lklhd     = lambda x: pdf_G_1(y,x,R)\n",
    "    \n",
    "    post_vals = Bayes_rule(prior(xx),lklhd(xx),xx[1]-xx[0])\n",
    "    mu, P     = Bayes_rule_Gaussian(b,B,y,R)\n",
    "    postr     = lambda x: pdf_G_1(x,mu,P)\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(xx,prior(xx)     ,label='prior N(x|0,1)')\n",
    "    plt.plot(xx,lklhd(xx)     ,label='likelihood N(y|x,R)')\n",
    "    plt.plot(xx,post_vals     ,label='posterior - pointwise')\n",
    "    plt.plot(xx,post_vals,'--',label='posterior - parametric')\n",
    "    plt.ylim(ymax=0.6)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** \n",
    " * Does the width (or scale) for the posterior depend on the location $y$ of the likelihood?\n",
    " * Is the width (or scale) for the posterior always smaller that that of prior and likelihood? What does this mean information-wise?\n",
    " * Is this always the case, also for non-Gaussian distributions?\n",
    " * What if you're pretty sure about something, and you get a wildly different indication (observation). What is your posterior certainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Why are we so fond of the Gaussian assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_answer('why Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Univariate (scalar) Kalman filtering](T3 - Univariate Kalman filtering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
